Query,GitHubUrl,Relevance,Code
group by count,https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/ouidb/pyauto/ouidb/config.py#L43-L50,3,"    def get_top_entries(self):
        query = 'select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;'
        try:
            db = self.get_db()
            results = db.execute(query)
            return [i for i in results]
        finally:
            db.close()"
underline text in label widget,https://github.com/dendory/menu3/blob/350414966e8f5c7737cd527369c8087f4f8f600b/menu3/menu3.py#L45-L49,3,
how to randomly pick a number,https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/utils.py#L515-L539,3,"def get_random_int(min_v=0, max_v=10, number=5, seed=None):
    """"""Return a list of random integer by the given range and quantity.

    Parameters
    -----------
    min_v : number
        The minimum value.
    max_v : number
        The maximum value.
    number : int
        Number of value.
    seed : int or None
        The seed for random.

    Examples
    ---------
    >>> r = get_random_int(min_v=0, max_v=10, number=5)
    [10, 2, 3, 3, 7]

    """"""
    rnd = random.Random()
    if seed:
        rnd = random.Random(seed)
    # return [random.randint(min,max) for p in range(0, number)]
    return [rnd.randint(min_v, max_v) for p in range(0, number)]"
encode url,https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/jwt.py#L52-L64,3,"    def _list(self, domain=None):
        # pylint: disable=arguments-differ
        url = ""{base}"".format(
            base=self.local_base_url
        )
        param = {}
        if domain:
            param['domainUuid'] = domain
        encode = urllib.urlencode(param)
        if encode:
            url += ""?""
            url += encode
        return self.core.list(url)"
connect to sql,https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/peewee.py#L3708-L3712,3,"    def _connect(self):
        if mysql is None:
            raise ImproperlyConfigured('MySQL driver not installed!')
        conn = mysql.connect(db=self.database, **self.connect_params)
        return conn"
binomial distribution,https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40,3,"def binomial(n,k):
    """"""
    Binomial coefficient
    >>> binomial(5,2)
    10
    >>> binomial(10,5)
    252
    """"""
    if n==k: return 1
    assert n>k, ""Attempting to call binomial(%d,%d)"" % (n,k)
    return factorial(n)//(factorial(k)*factorial(n-k))"
get the description of a http status code,https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54,3,"def get_description(status_code):
    """"""
    Get the description for a status code.
    """"""
    description = _descriptions.get(status_code)
    if description is None:
        description = 'code = %s (no description)' % str(status_code)
    return description"
parse binary file to custom class,https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/file.py#L109-L114,3,"    def binary_file(self, file=None):
        """"""Same as :meth:`file` but for binary content.""""""
        if file is None:
            file = BytesIO()
        self._binary_file(file)
        return file"
pretty print json,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/utils.py#L112-L121,3,"def pprint(j, no_pretty):
    """"""
    Prints as formatted JSON
    """"""
    if not no_pretty:
        click.echo(
            json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=("","", "": ""))
        )
    else:
        click.echo(j)"
deserialize json,https://github.com/ubc/github2gitlab/blob/795898f6d438621fa0c996a7156d70c382ff0493/github2gitlab/main.py#L434-L440,3,"    def json_loads(payload):
        ""Log the payload that cannot be parsed""
        try:
            return json.loads(payload)
        except ValueError as e:
            log.error(""unable to json.loads("" + payload + "")"")
            raise e"
postgresql connection,https://github.com/gabfl/dbschema/blob/37722e6654e9f0374fac5518ebdca22f4c39f92f/src/schema_change.py#L81-L91,3,"def get_connection(engine, host, user, port, password, database, ssl={}):
    """""" Returns a PostgreSQL or MySQL connection """"""

    if engine == 'mysql':
        # Connection
        return get_mysql_connection(host, user, port, password, database, ssl)
    elif engine == 'postgresql':
        # Connection
        return get_pg_connection(host, user, port, password, database, ssl)
    else:
        raise RuntimeError('`%s` is not a valid engine.' % engine)"
how to make the checkbox checked,https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667,3,"    def checkbox_uncheck(self, force_check=False):
        """"""
        Wrapper to uncheck a checkbox
        """"""
        if self.get_attribute('checked'):
            self.click(force_click=force_check)"
copy to clipboard,https://github.com/manns/pyspread/blob/0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0/pyspread/src/gui/_main_window.py#L1289-L1305,3,
httpclient post json,https://github.com/rehive/rehive-python/blob/a7452a9cfecf76c5c8f0d443f122ed22167fb164/rehive/api/client.py#L50-L51,3,"    def post(self, path, data, json=True, **kwargs):
        return self._request('post', path, data, json=json, **kwargs)"
string to date,https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/mlb/schedule.py#L172-L179,3,"    def datetime(self):
        """"""
        Returns a datetime object of the month, day, year, and time the game
        was played.
        """"""
        date_string = '%s %s' % (self._date, self._year)
        date_string = re.sub(r' \(\d+\)', '', date_string)
        return datetime.strptime(date_string, '%A, %b %d %Y')"
write csv,https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518,3,"    def write_to_csv(self, filename):
        '''
        Exports to simple csv

        :param str filename:
            Path to file for export
        '''
        fid = open(filename, 'wt')
        # Create header list
        header_info = ['Longitude', 'Latitude', 'Depth', 'Observed Count',
                       'Smoothed Rate', 'b-value']
        writer = csv.DictWriter(fid, fieldnames=header_info)
        headers = dict((name0, name0) for name0 in header_info)
        # Write to file
        writer.writerow(headers)
        for row in self.data:
            # institute crude compression by omitting points with no seismicity
            # and taking advantage of the %g format
            if row[4] == 0:
                continue
            row_dict = {'Longitude': '%g' % row[0],
                        'Latitude': '%g' % row[1],
                        'Depth': '%g' % row[2],
                        'Observed Count': '%d' % row[3],
                        'Smoothed Rate': '%.6g' % row[4],
                        'b-value': '%g' % self.bval}
            writer.writerow(row_dict)
        fid.close()"
how to check if a checkbox is checked,https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L359-L361,3,"def assert_not_checked_checkbox(step, value):
    check_box = find_field(world.browser, 'checkbox', value)
    assert_true(step, not check_box.is_selected())"
string similarity levenshtein,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/functions.py#L1634-L1643,3,"def levenshtein(left, right):
    """"""Computes the Levenshtein distance of the two given strings.

    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])
    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()
    [Row(d=3)]
    """"""
    sc = SparkContext._active_spark_context
    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))
    return Column(jc)"
parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_rdf.py#L9-L17,3,"def parse_command_line_arguments():
    # command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument( 'xdatcar' )
    parser.add_argument( 'label', nargs = 2 )
    parser.add_argument( 'max_r', type = float )
    parser.add_argument( 'n_bins', type = int )
    args = parser.parse_args()
    return( args )"
unzipping large files,https://github.com/JIC-CSB/jicbioimage.transform/blob/494c282d964c3a9b54c2a1b3730f5625ea2a494b/appveyor/install.py#L19-L23,3,"def unzip_file(zip_fname):
    """"""Unzip the zip_fname in the current directory."""""" 
    print(""Unzipping {}"".format(zip_fname))
    with zipfile.ZipFile(zip_fname) as zf:
        zf.extractall()"
replace in file,https://github.com/riptano/ccm/blob/275699f79d102b5039b79cc17fa6305dccf18412/ccmlib/common.py#L208-L219,3,"def replaces_in_file(file, replacement_list):
    rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list]
    file_tmp = file + ""."" + str(os.getpid()) + "".tmp""
    with open(file, 'r') as f:
        with open(file_tmp, 'w') as f_tmp:
            for line in f:
                for r, replace in rs:
                    match = r.search(line)
                    if match:
                        line = replace + ""\n""
                f_tmp.write(line)
    shutil.move(file_tmp, file)"
filter array,https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463,3,"    def params(self):
        if not any(filter.params for filter in self):
            return None
        else:
            return Array(filter.params or Null() for filter in self)"
all permutations of a list,https://github.com/erikrose/more-itertools/blob/6a91b4e25c8e12fcf9fc2b53cf8ee0fba293e6f9/more_itertools/more.py#L528-L566,3,"def distinct_permutations(iterable):
    """"""Yield successive distinct permutations of the elements in *iterable*.

        >>> sorted(distinct_permutations([1, 0, 1]))
        [(0, 1, 1), (1, 0, 1), (1, 1, 0)]

    Equivalent to ``set(permutations(iterable))``, except duplicates are not
    generated and thrown away. For larger input sequences this is much more
    efficient.

    Duplicate permutations arise when there are duplicated elements in the
    input iterable. The number of items returned is
    `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of
    items input, and each `x_i` is the count of a distinct item in the input
    sequence.

    """"""
    def make_new_permutations(permutations, e):
        """"""Internal helper function.
        The output permutations are built up by adding element *e* to the
        current *permutations* at every possible position.
        The key idea is to keep repeated elements (reverse) ordered:
        if e1 == e2 and e1 is before e2 in the iterable, then all permutations
        with e1 before e2 are ignored.

        """"""
        for permutation in permutations:
            for j in range(len(permutation)):
                yield permutation[:j] + [e] + permutation[j:]
                if permutation[j] == e:
                    break
            else:
                yield permutation + [e]

    permutations = [[]]
    for e in iterable:
        permutations = make_new_permutations(permutations, e)

    return (tuple(t) for t in permutations)"
k means clustering,https://github.com/insilicolife/micti/blob/f12f46724295b57c4859e6acf7eab580fc355eb1/build/lib/MICTI/GM.py#L71-L87,3,
unique elements,https://github.com/matthew-brett/delocate/blob/ed48de15fce31c3f52f1a9f32cae1b02fc55aa60/delocate/tools.py#L72-L89,3,"def unique_by_index(sequence):
    """""" unique elements in `sequence` in the order in which they occur

    Parameters
    ----------
    sequence : iterable

    Returns
    -------
    uniques : list
        unique elements of sequence, ordered by the order in which the element
        occurs in `sequence`
    """"""
    uniques = []
    for element in sequence:
        if element not in uniques:
            uniques.append(element)
    return uniques"
get executable path,https://github.com/etcher-be/elib_run/blob/c9d8ba9f067ab90c5baa27375a92b23f1b97cdde/elib_run/_find_exe.py#L18-L62,3,"def find_executable(executable: str, *paths: str) -> typing.Optional[Path]:
    """"""
    Based on: https://gist.github.com/4368898

    Public domain code by anatoly techtonik <techtonik@gmail.com>

    Programmatic equivalent to Linux `which` and Windows `where`

    Find if ´executable´ can be run. Looks for it in 'path'
    (string that lists directories separated by 'os.pathsep';
    defaults to os.environ['PATH']). Checks for all executable
    extensions. Returns full path or None if no command is found.

    Args:
        executable: executable name to look for
        paths: root paths to examine (defaults to system PATH)

    Returns: executable path as string or None

    """"""

    if not executable.endswith('.exe'):
        executable = f'{executable}.exe'

    if executable in _KNOWN_EXECUTABLES:
        return _KNOWN_EXECUTABLES[executable]

    output = f'{executable}'

    if not paths:
        path = os.environ['PATH']
        paths = tuple([str(Path(sys.exec_prefix, 'Scripts').absolute())] + path.split(os.pathsep))
    executable_path = Path(executable).absolute()
    if not executable_path.is_file():
        for path_ in paths:
            executable_path = Path(path_, executable).absolute()
            if executable_path.is_file():
                break
        else:
            _LOGGER.error('%s -> not found', output)
            return None

    _KNOWN_EXECUTABLES[executable] = executable_path
    _LOGGER.info('%s -> %s', output, str(executable_path))
    return executable_path"
format date,https://github.com/geex-arts/django-jet/blob/64d5379f8a2278408694ce7913bf25f26035b855/jet/dashboard/dashboard_modules/google_analytics.py#L267-L279,3,"    def format_grouped_date(self, data, group):
        date = self.get_grouped_date(data, group)

        if group == 'week':
            date = u'%s — %s' % (
                (date - datetime.timedelta(days=6)).strftime('%d.%m'),
                date.strftime('%d.%m')
            )
        elif group == 'month':
            date = date.strftime('%b, %Y')
        else:
            date = formats.date_format(date, 'DATE_FORMAT')
        return date"
how to read the contents of a .gz compressed file?,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/lib/upload.py#L225-L246,3,"def _file_size(file_path, uncompressed=False):
    """"""Return size of a single file, compressed or uncompressed""""""
    _, ext = os.path.splitext(file_path)

    if uncompressed:
        if ext in {"".gz"", "".gzip""}:
            with gzip.GzipFile(file_path, mode=""rb"") as fp:
                try:
                    fp.seek(0, os.SEEK_END)
                    return fp.tell()
                except ValueError:
                    # on python2, cannot seek from end and must instead read to end
                    fp.seek(0)
                    while len(fp.read(8192)) != 0:
                        pass
                    return fp.tell()
        elif ext in {"".bz"", "".bz2"", "".bzip"", "".bzip2""}:
            with bz2.BZ2File(file_path, mode=""rb"") as fp:
                fp.seek(0, os.SEEK_END)
                return fp.tell()

    return os.path.getsize(file_path)"
finding time elapsed using a timer,https://github.com/alexmojaki/littleutils/blob/1132d2d2782b05741a907d1281cd8c001f1d1d9d/littleutils/__init__.py#L716-L739,3,"def timer(description='Operation', log=None):
    """"""
    Simple context manager which logs (if log is provided)
    or prints the time taken in seconds for the block to complete.

    >>> with timer():
    ...    sleep(0.1)               # doctest:+ELLIPSIS
    Operation took 0.1... seconds

    >>> with timer('Sleeping'):
    ...    sleep(0.2)               # doctest:+ELLIPSIS
    Sleeping took 0.2... seconds

    >>> with timer(description='Doing', log=PrintingLogger()):
    ...    sleep(0.3)               # doctest:+ELLIPSIS
    Doing took 0.3... seconds

    """"""

    start = time()
    yield
    elapsed = time() - start
    message = '%s took %s seconds' % (description, elapsed)
    (print if log is None else log.info)(message)"
get name of enumerated value,https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L775-L785,3,"    def _to_string(self):
        enum_name = None
        value = self._get_calculated_value(self.value)
        for enum, enum_value in vars(self.enum_type).items():
            if value == enum_value:
                enum_name = enum
                break
        if enum_name is None:
            return ""(%d) UNKNOWN_ENUM"" % value
        else:
            return ""(%d) %s"" % (value, enum_name)"
randomly extract x items from a list,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_list.py#L2155-L2189,3,"def sample_lists(items_list, num=1, seed=None):
    r""""""
    Args:
        items_list (list):
        num (int): (default = 1)
        seed (None): (default = None)

    Returns:
        list: samples_list

    CommandLine:
        python -m utool.util_list --exec-sample_lists

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_list import *  # NOQA
        >>> items_list = [[], [1, 2, 3], [4], [5, 6], [7, 8, 9, 10]]
        >>> num = 2
        >>> seed = 0
        >>> samples_list = sample_lists(items_list, num, seed)
        >>> result = ('samples_list = %s' % (str(samples_list),))
        >>> print(result)
        samples_list = [[], [3, 2], [4], [5, 6], [10, 9]]
    """"""
    if seed is not None:
        rng = np.random.RandomState(seed)
    else:
        rng = np.random
    def random_choice(items, num):
        size = min(len(items), num)
        return rng.choice(items, size, replace=False).tolist()
    samples_list = [random_choice(items, num)
                    if len(items) > 0 else []
                    for items in items_list]
    return samples_list"
convert int to string,https://github.com/DeepHorizons/iarm/blob/b913c9fd577b793a6bbced78b78a5d8d7cd88de4/iarm/arm_instructions/_meta.py#L126-L132,3,"    def convert_to_integer(self, str):
        if str.startswith('0x') or str.startswith('0X'):
            return int(str, 16)
        elif str.startswith('2_'):
            return int(str[2:], 2)
        else:
            return int(str)"
sending binary data over a serial connection,https://github.com/Seeed-Studio/wio-cli/blob/ce83f4c2d30be7f72d1a128acd123dfc5effa563/wio/commands/cmd_setup.py#L217-L384,3,"def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port):
    ### check is configure mode?
    thread = termui.waiting_echo(""Getting device information..."")
    thread.daemon = True
    thread.start()

    flag = False
    try:
        with serial.Serial(port, 115200, timeout=5) as ser:
            cmd = 'Blank?\r\n'
            ser.write(cmd.encode('utf-8'))
            if 'Node' in ser.readline():
                flag = True
    except serial.SerialException as e:
        thread.stop('')
        thread.join()
        click.secho('>> ', fg='red', nl=False)
        click.echo(e)
        if e.errno == 13:
            click.echo(""For more information, see https://github.com/Seeed-Studio/wio-cli#serial-port-permissions"")
        return None

    thread.stop('')
    thread.join()

    if flag:
        click.secho('> ', fg='green', nl=False)
        click.secho(""Found Wio."", fg='green', bold=True)
        click.echo()
    else:
        click.secho('> ', fg='green', nl=False)
        click.secho(""No nearby Wio detected."", fg='white', bold=True)
        if click.confirm(click.style('? ', fg='green') +
                click.style(""Would you like to wait and monitor for Wio entering configure mode"", bold=True),
                default=True):

            thread = termui.waiting_echo(""Waiting for a wild Wio to appear... (press ctrl + C to exit)"")
            thread.daemon = True
            thread.start()

            flag = False
            while 1:
                with serial.Serial(port, 115200, timeout=5) as ser:
                    cmd = 'Blank?\r\n'
                    ser.write(cmd.encode('utf-8'))
                    if 'Node' in ser.readline():
                        flag = True
                        break

            thread.stop('')
            thread.join()
            click.secho('> ', fg='green', nl=False)
            click.secho(""Found Wio."", fg='green', bold=True)
            click.echo()
        else:
            click.secho('> ', fg='green', nl=False)
            click.secho(""\nQuit wio setup!"", bg='white', bold=True)

    while 1:
        if not click.confirm(click.style('? ', fg='green') +
                    click.style(""Would you like to manually enter your Wi-Fi network configuration?"", bold=True),
                    default=False):
            thread = termui.waiting_echo(""Asking the Wio to scan for nearby Wi-Fi networks..."")
            thread.daemon = True
            thread.start()

            flag = False
            with serial.Serial(port, 115200, timeout=3) as ser:
                cmd = 'SCAN\r\n'
                ser.write(cmd.encode('utf-8'))
                ssid_list = []
                while True:
                    ssid = ser.readline()
                    if ssid == '\r\n':
                        flag = True
                        break
                    ssid = ssid.strip('\r\n')
                    ssid_list.append(ssid)

            if flag:
                thread.stop('')
                thread.join()
            else:
                thread.stop(""\rsearch failure...\n"")
                return None

            while 1:
                for x in range(len(ssid_list)):
                    click.echo(""%s.) %s"" %(x, ssid_list[x]))
                click.secho('? ', fg='green', nl=False)
                value = click.prompt(
                            click.style('Please select the network to which your Wio should connect', bold=True),
                            type=int)
                if value >= 0 and value < len(ssid_list):
                    ssid = ssid_list[value]
                    break
                else:
                    click.echo(click.style('>> ', fg='red') + ""invalid input, range 0 to %s"" %(len(ssid_list)-1))

            ap = ssid
        else:
            ap = click.prompt(click.style('> ', fg='green') +
                click.style('Please enter the SSID of your Wi-Fi network', bold=True), type=str)

        ap_pwd = click.prompt(click.style('> ', fg='green') +
            click.style('Please enter your Wi-Fi network password (leave blank for none)', bold=True),
            default='', show_default=False)
        d_name = click.prompt(click.style('> ', fg='green') +
        click.style('Please enter the name of a device will be created', bold=True), type=str)

        click.echo(click.style('> ', fg='green') + ""Here's what we're going to send to the Wio:"")
        click.echo()
        click.echo(click.style('> ', fg='green') + ""Wi-Fi network: "" +
            click.style(ap, fg='green', bold=True))
        ap_pwd_p = ap_pwd
        if ap_pwd_p == '':
            ap_pwd_p = 'None'
        click.echo(click.style('> ', fg='green') + ""Password: "" +
            click.style(ap_pwd_p, fg='green', bold=True))
        click.echo(click.style('> ', fg='green') + ""Device name: "" +
            click.style(d_name, fg='green', bold=True))
        click.echo()

        if click.confirm(click.style('? ', fg='green') +
            ""Would you like to continue with the information shown above?"", default=True):
            break

    click.echo()
    #waiting ui
    thread = termui.waiting_echo(""Sending Wi-Fi information to device..."")
    thread.daemon = True
    thread.start()

    # send serial command
    ## get version
    version = 1.1
    with serial.Serial(port, 115200, timeout=10) as ser:
        cmd = 'VERSION\r\n'
        ser.write(cmd.encode('utf-8'))
        res = ser.readline()
        try:
            version = float(re.match(r""([0-9]+.[0-9]+)"", res).group(0))
        except Exception as e:
            version = 1.1

    send_flag = False
    while 1:
        with serial.Serial(port, 115200, timeout=10) as ser:
            if version <= 1.1:
                cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr_ip, msvr_ip)
            elif version >= 1.2:
                cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)
            else:
                cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)
            # click.echo(cmd)
            ser.write(cmd.encode('utf-8'))
            if ""ok"" in ser.readline():
                click.echo(click.style('\r> ', fg='green') + ""Send Wi-Fi information to device success."")
                thread.stop('')
                thread.join()
                send_flag = True
        if send_flag:
            break

    if send_flag:
        return {'name': d_name}
    else:
        return None"
scatter plot,https://github.com/newville/wxmplot/blob/8e0dc037453e5cdf18c968dc5a3d29efd761edee/wxmplot/plotframe.py#L45-L47,3,"    def scatterplot(self, x, y, **kw):
        """"""plot after clearing current plot """"""
        self.panel.scatterplot(x, y, **kw)"
convert a date string into yyyymmdd,https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82,3,"def day_to_month(timeperiod):
    """""":param timeperiod: as string in YYYYMMDD00 format
    :return string in YYYYMM0000 format""""""
    t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)
    return t.strftime(SYNERGY_MONTHLY_PATTERN)"
how to extract zip file recursively,https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/utils.py#L43-L65,3,"def extract_zipdir(zip_file):
    """"""
    Extract contents of zip file into subfolder in parent directory.
    
    Parameters
    ----------
    zip_file : str
        Path to zip file
    
    Returns
    -------
        str : folder where the zip was extracted
    """"""
    if not os.path.exists(zip_file):
        raise ValueError('{} does not exist'.format(zip_file))
    directory = os.path.dirname(zip_file)
    filename = os.path.basename(zip_file)
    dirpath = os.path.join(directory, filename.replace('.zip', ''))

    with zipfile.ZipFile(zip_file, 'r', zipfile.ZIP_DEFLATED) as zipf:
        zipf.extractall(dirpath)

    return dirpath"
aes encryption,https://github.com/kalefranz/auxlib/blob/6ff2d6b57d128d0b9ed8f01ad83572e938da064f/auxlib/crypt.py#L77-L97,3,"def aes_encrypt(base64_encryption_key, data):
    """"""Encrypt data with AES-CBC and sign it with HMAC-SHA256

    Arguments:
        base64_encryption_key (str): a base64-encoded string containing an AES encryption key
            and HMAC signing key as generated by generate_encryption_key()
        data (str): a byte string containing the data to be encrypted

    Returns:
        str: the encrypted data as a byte string with the HMAC signature appended to the end

    """"""
    if isinstance(data, text_type):
        data = data.encode(""UTF-8"")
    aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)
    data = _pad(data)
    iv_bytes = os.urandom(AES_BLOCK_SIZE)
    cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)
    data = iv_bytes + cipher.encrypt(data)  # prepend init vector
    hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()
    return as_base64(data + hmac_signature)"
extract data from html content,https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/html_metadata_extractor.py#L40-L90,3,"    def extract(self, html_text: str,
                extract_title: bool = False,
                extract_meta: bool = False,
                extract_microdata: bool = False,
                microdata_base_url: str = """",
                extract_json_ld: bool = False,
                extract_rdfa: bool = False,
                rdfa_base_url: str = """") \
            -> List[Extraction]:
        """"""
        Args:
            html_text (str): input html string to be extracted
            extract_title (bool): True if string of 'title' tag needs to be extracted, return as { ""title"": ""..."" }
            extract_meta (bool): True if string of 'meta' tags needs to be extracted, return as { ""meta"": { ""author"": ""..."", ...}}
            extract_microdata (bool): True if microdata needs to be extracted, returns as { ""microdata"": [...] }
            microdata_base_url (str): base namespace url for microdata, empty string if no base url is specified
            extract_json_ld (bool): True if json-ld needs to be extracted, return as { ""json-ld"": [...] }
            extract_rdfa (bool): True if rdfs needs to be extracted, returns as { ""rdfa"": [...] }
            rdfa_base_url (str): base namespace url for rdfa, empty string if no base url is specified

        Returns:
            List[Extraction]: the list of extraction or the empty list if there are no matches.
        """"""
        res = list()
        soup = BeautifulSoup(html_text, 'html.parser')

        if soup.title and extract_title:
            title = self._wrap_data(""title"", soup.title.string.encode('utf-8').decode('utf-8'))
            res.append(title)

        if soup.title and extract_meta:
            meta_content = self._wrap_meta_content(soup.find_all(""meta""))
            meta_data = self._wrap_data(""meta"", meta_content)
            res.append(meta_data)

        if extract_microdata:
            mde = MicrodataExtractor()
            mde_data = self._wrap_data(""microdata"", mde.extract(html_text, microdata_base_url))
            res.append(mde_data)

        if extract_json_ld:
            jslde = JsonLdExtractor()
            jslde_data = self._wrap_data(""json-ld"", jslde.extract(html_text))
            res.append(jslde_data)

        if extract_rdfa:
            rdfae = RDFaExtractor()
            rdfae_data = self._wrap_data(""rdfa"", rdfae.extract(html_text, rdfa_base_url))
            res.append(rdfae_data)

        return res"
create cookie,https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L299-L342,3,"    def create_cookie(self, value, typ, cookie_name=None, ttl=-1, kill=False):
        """"""

        :param value: Part of the cookie payload
        :param typ: Type of cookie
        :param cookie_name:
        :param ttl: Number of minutes before this cookie goes stale
        :param kill: Whether the the cookie should expire on arrival
        :return: A tuple to be added to headers
        """"""
        if kill:
            ttl = -1
        elif ttl < 0:
            ttl = self.default_value['max_age']

        if cookie_name is None:
            cookie_name = self.default_value['name']

        c_args = {}

        srvdomain = self.default_value['domain']
        if srvdomain and srvdomain not in ['localhost', '127.0.0.1',
                                           '0.0.0.0']:
            c_args['domain'] = srvdomain

        srvpath = self.default_value['path']
        if srvpath:
            c_args['path'] = srvpath

        # now
        timestamp = str(int(time.time()))

        # create cookie payload
        try:
            cookie_payload = ""::"".join([value, timestamp, typ])
        except TypeError:
            cookie_payload = ""::"".join([value[0], timestamp, typ])

        cookie = make_cookie(
            cookie_name, cookie_payload, self.sign_key,
            timestamp=timestamp, enc_key=self.enc_key, max_age=ttl,
            sign_alg=self.sign_alg, **c_args)

        return cookie"
convert int to bool,https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/arguments.py#L20-L31,3,"def boolval(v):
    if isinstance(v, bool):
        return v
    if isinstance(v, int):
        return bool(v)
    if is_string(v):
        v = v.lower()
        if v in {'j', 'y', 'ja', 'yes', '1', 'true'}:
            return True
        if v in {'n', 'nei', 'no', '0', 'false'}:
            return False
    raise ValueError(""Don't know how to convert %r to bool"" % v)"
memoize to disk  - persistent memoization,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_decor.py#L601-L651,3,"def memoize(func):
    """"""
    simple memoization decorator

    References:
        https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize

    Args:
        func (function):  live python function

    Returns:
        func:

    CommandLine:
        python -m utool.util_decor memoize

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_decor import *  # NOQA
        >>> import utool as ut
        >>> closure = {'a': 'b', 'c': 'd'}
        >>> incr = [0]
        >>> def foo(key):
        >>>     value = closure[key]
        >>>     incr[0] += 1
        >>>     return value
        >>> foo_memo = memoize(foo)
        >>> assert foo('a') == 'b' and foo('c') == 'd'
        >>> assert incr[0] == 2
        >>> print('Call memoized version')
        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'
        >>> assert incr[0] == 4
        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'
        >>> print('Counter should no longer increase')
        >>> assert incr[0] == 4
        >>> print('Closure changes result without memoization')
        >>> closure = {'a': 0, 'c': 1}
        >>> assert foo('a') == 0 and foo('c') == 1
        >>> assert incr[0] == 6
        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'
    """"""
    cache = func._util_decor_memoize_cache = {}
    # @functools.wraps(func)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]
    memoizer = preserve_sig(memoizer, func)
    memoizer.cache = cache
    return memoizer"
get current ip address,https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/mover.py#L203-L208,3,"    def get_ip(self):
        if self._ip is not None:
            ret = self._ip
        else:
            ret = self.ip_addr
        return ret"
read text file line by line,https://github.com/chrisjsewell/jsonextended/blob/c3a7a880cc09789b3c61204265dcbb127be76c8a/jsonextended/mockpath.py#L44-L54,3,"    def readline(self):
        if self._current_line >= len(self._linelist):
            line = ''
        else:
            line = self._linelist[self._current_line] + '\n'
        self._current_line += 1
        self._current_indx += len(
            '\n'.join(self._linelist[0:self._current_line]))
        if self._encoding is not None:
            line = line.encode(self._encoding)
        return line"
socket recv timeout,https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/isotp.py#L612-L621,3,"    def recv_with_timeout(self, timeout=1):
        """"""Receive a complete ISOTP message, blocking until a message is
        received or the specified timeout is reached.
        If timeout is 0, then this function doesn't block and returns the
        first frame in the receive buffer or None if there isn't any.""""""
        msg = self.ins.recv(timeout)
        t = time.time()
        if msg is None:
            raise Scapy_Exception(""Timeout"")
        return self.basecls, msg, t"
regex case insensitive,https://github.com/cltk/cltk/blob/ed9c025b7ec43c949481173251b70e05e4dffd27/cltk/ir/query.py#L24-L37,3,"def _regex_span(_regex, _str, case_insensitive=True):
    """"""Return all matches in an input string.
    :rtype : regex.match.span
    :param _regex: A regular expression pattern.
    :param _str: Text on which to run the pattern.
    """"""
    if case_insensitive:
        flags = regex.IGNORECASE | regex.FULLCASE | regex.VERSION1
    else:
        flags = regex.VERSION1
    comp = regex.compile(_regex, flags=flags)
    matches = comp.finditer(_str)
    for match in matches:
        yield match"
custom http error response,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/vendored/potion_client/links.py#L73-L90,3,"    def raise_for_status(self, response):
        http_error_msg = ''

        if 400 <= response.status_code < 500:
            try:
                http_error_msg = response.json()
            except:
                http_error_msg = ('{code} Client Error: {reason} for url: {url}'.format(
                    code=response.status_code, reason=response.reason, url=response.url)
                )

        elif 500 <= response.status_code < 600:
            http_error_msg = ('{code} Server Error: {reason} for url: {url}'.format(
                code=response.status_code, reason=response.reason, url=response.url)
            )

        if http_error_msg:
            raise HTTPError(http_error_msg, response=response)"
parse json file,https://github.com/joeferraro/mm/blob/43dce48a2249faab4d872c228ada9fbdbeec147b/mm/util.py#L110-L120,3,
convert json to csv,https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47,3,"def json_to_csv(json_input):
    '''
    Convert simple JSON to CSV
    Accepts a JSON string or JSON object
    '''
    try:
        json_input = json.loads(json_input)
    except:
        pass # If loads fails, it's probably already parsed
    headers = set()
    for json_row in json_input:
        headers.update(json_row.keys())

    csv_io = StringIO.StringIO()
    csv_out = csv.DictWriter(csv_io,headers)
    csv_out.writeheader()
    for json_row in json_input:
        csv_out.writerow(json_row)
    csv_io.seek(0)
    return csv_io.read()"
extract latitude and longitude from given input,https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16,3,"    def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):
        """"""Creates a point from lat/lon in WGS84""""""
        assert -180.0 <= longitude <= 180.0, 'Longitude needs to be a value between -180.0 and 180.0.'
        assert -90.0 <= latitude <= 90.0, 'Latitude needs to be a value between -90.0 and 90.0.'
        return cls(latitude=latitude, longitude=longitude)"
set working directory,https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/dynamips/dynamips_hypervisor.py#L152-L162,3,"    def set_working_dir(self, working_dir):
        """"""
        Sets the working directory for this hypervisor.

        :param working_dir: path to the working directory
        """"""

        # encase working_dir in quotes to protect spaces in the path
        yield from self.send('hypervisor working_dir ""{}""'.format(working_dir))
        self._working_dir = working_dir
        log.debug(""Working directory set to {}"".format(self._working_dir))"
export to excel,https://github.com/fkarb/xltable/blob/7a592642d27ad5ee90d2aa8c26338abaa9d84bea/xltable/workbook.py#L82-L123,3,"    def to_excel(self, xl_app=None, resize_columns=True):
        from win32com.client import Dispatch, gencache

        if xl_app is None:
            xl_app = Dispatch(""Excel.Application"")
        xl_app = gencache.EnsureDispatch(xl_app)

        # Add a new workbook with the correct number of sheets.
        # We aren't allowed to create an empty one.
        assert self.worksheets, ""Can't export workbook with no worksheets""
        sheets_in_new_workbook = xl_app.SheetsInNewWorkbook
        try:
            xl_app.SheetsInNewWorkbook = float(len(self.worksheets))
            self.workbook_obj = xl_app.Workbooks.Add()
        finally:
            xl_app.SheetsInNewWorkbook = sheets_in_new_workbook

        # Rename the worksheets, ensuring that there can never be two sheets with the same
        # name due to the sheets default names conflicting with the new names.
        sheet_names = {s.name for s in self.worksheets}
        assert len(sheet_names) == len(self.worksheets), ""Worksheets must have unique names""
        for worksheet in self.workbook_obj.Sheets:
            i = 1
            original_name = worksheet.Name
            while worksheet.Name in sheet_names:
                worksheet.Name = ""%s_%d"" % (original_name, i)
                i += 1

        for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets):
            worksheet.Name = sheet.name

        # Export each sheet (have to use itersheets for this as it sets the
        # current active sheet before yielding each one).
        for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()):
            worksheet.Select()
            sheet.to_excel(workbook=self,
                           worksheet=worksheet,
                           xl_app=xl_app,
                           rename=False,
                           resize_columns=resize_columns)

        return self.workbook_obj"
how to read .csv file in an efficient way?,https://github.com/remix/partridge/blob/0ba80fa30035e5e09fd8d7a7bdf1f28b93d53d03/partridge/gtfs.py#L89-L112,3,"    def _read_csv(self, filename: str) -> pd.DataFrame:
        path = self._pathmap.get(filename)
        columns = self._config.nodes.get(filename, {}).get(""required_columns"", [])

        if path is None or os.path.getsize(path) == 0:
            # The file is missing or empty. Return an empty
            # DataFrame containing any required columns.
            return empty_df(columns)

        # If the file isn't in the zip, return an empty DataFrame.
        with open(path, ""rb"") as f:
            encoding = detect_encoding(f)

        df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False)

        # Strip leading/trailing whitespace from column names
        df.rename(columns=lambda x: x.strip(), inplace=True)

        if not df.empty:
            # Strip leading/trailing whitespace from column values
            for col in df.columns:
                df[col] = df[col].str.strip()

        return df"
how to get html of website,https://github.com/dwillis/python-espncricinfo/blob/96469e39f309e28586fcec40cc6a20b2fddacff7/espncricinfo/player.py#L44-L50,3,"    def get_html(self):
        r = requests.get(self.url)
        if r.status_code == 404:
            raise PlayerNotFoundError
        else:
            soup = BeautifulSoup(r.text, 'html.parser')
            return soup.find(""div"", class_=""pnl490M"")"
get all parents of xml node,https://github.com/fishtown-analytics/dbt/blob/aa4f771df28b307af0cf9fe2fc24432f10a8236b/core/dbt/graph/selector.py#L317-L344,3,"    def get_ancestor_ephemeral_nodes(self, selected_nodes):
        node_names = {}
        for node_id in selected_nodes:
            if node_id not in self.manifest.nodes:
                continue
            node = self.manifest.nodes[node_id]
            # sources don't have ancestors and this results in a silly select()
            if node.resource_type == NodeType.Source:
                continue
            node_names[node_id] = node.name

        include_spec = [
            '+{}'.format(node_names[node])
            for node in selected_nodes if node in node_names
        ]
        if not include_spec:
            return set()

        all_ancestors = self.select_nodes(self.linker.graph, include_spec, [])

        res = []
        for ancestor in all_ancestors:
            ancestor_node = self.manifest.nodes.get(ancestor, None)

            if ancestor_node and self.is_ephemeral_model(ancestor_node):
                res.append(ancestor)

        return set(res)"
convert html to pdf,https://github.com/thewca/wca-regulations-compiler/blob/3ebbd8fe8fec7c9167296f59b2677696fe61a954/wrc/wrc.py#L85-L113,3,"def html_to_pdf(tmp_filenames, output_directory, lang_options):
    input_html = output_directory + ""/"" + tmp_filenames[0]
    wkthml_cmd = [""wkhtmltopdf""]
    # Basic margins etc
    wkthml_cmd.extend([""--margin-left"", ""18""])
    wkthml_cmd.extend([""--margin-right"", ""18""])
    wkthml_cmd.extend([""--page-size"", ""Letter""])
    # Header and Footer
    header_file = pkg_resources.resource_filename(""wrc"", ""data/header.html"")
    footer_file = pkg_resources.resource_filename(""wrc"", ""data/footer.html"")
    wkthml_cmd.extend([""--header-html"", header_file])
    wkthml_cmd.extend([""--footer-html"", footer_file])
    wkthml_cmd.extend([""--header-spacing"", ""8""])
    wkthml_cmd.extend([""--footer-spacing"", ""8""])
    wkthml_cmd.append(input_html)
    wkthml_cmd.append(output_directory + ""/"" + lang_options['pdf'] + '.pdf')
    try:
        check_call(wkthml_cmd)
        print ""Successfully generated pdf file!""
        print ""Cleaning temporary file (%s)..."" % input_html
        os.remove(input_html)
    except CalledProcessError as err:
        print ""Error while generating pdf:""
        print err
        sys.exit(1)
    except OSError as err:
        print ""Error when running command \"""" + "" "".join(wkthml_cmd) + ""\""""
        print err
        sys.exit(1)"
linear regression,https://github.com/ulf1/oxyba/blob/b3043116050de275124365cb11e7df91fb40169d/oxyba/linreg_mle.py#L2-L59,3,
how to get database table name,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L578-L591,3,"    def get_table(self, table_name, db='default'):
        """"""Get a metastore table object

        >>> hh = HiveMetastoreHook()
        >>> t = hh.get_table(db='airflow', table_name='static_babynames')
        >>> t.tableName
        'static_babynames'
        >>> [col.name for col in t.sd.cols]
        ['state', 'year', 'name', 'gender', 'num']
        """"""
        if db == 'default' and '.' in table_name:
            db, table_name = table_name.split('.')[:2]
        with self.metastore as client:
            return client.get_table(dbname=db, tbl_name=table_name)"
encrypt aes ctr mode,https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L59-L63,3,"    def aes_ctr_encrypt(plain_text: bytes, key: bytes):
        cipher = AES.new(key=key, mode=AES.MODE_CTR)
        cipher_text = cipher.encrypt(plain_text)
        nonce = cipher.nonce
        return nonce, cipher_text"
print model summary,https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/util/summary.py#L11-L86,3,"def summary(model, input_size):
    """""" Print summary of the model """"""
    def register_hook(module):
        def hook(module, input, output):
            class_name = str(module.__class__).split('.')[-1].split(""'"")[0]
            module_idx = len(summary)

            m_key = '%s-%i' % (class_name, module_idx + 1)
            summary[m_key] = OrderedDict()
            summary[m_key]['input_shape'] = list(input[0].size())
            summary[m_key]['input_shape'][0] = -1
            if isinstance(output, (list, tuple)):
                summary[m_key]['output_shape'] = [[-1] + list(o.size())[1:] for o in output]
            else:
                summary[m_key]['output_shape'] = list(output.size())
                summary[m_key]['output_shape'][0] = -1

            params = 0
            if hasattr(module, 'weight') and hasattr(module.weight, 'size'):
                params += torch.prod(torch.LongTensor(list(module.weight.size())))
                summary[m_key]['trainable'] = module.weight.requires_grad
            if hasattr(module, 'bias') and hasattr(module.bias, 'size'):
                params += torch.prod(torch.LongTensor(list(module.bias.size())))
            summary[m_key]['nb_params'] = params

        if (not isinstance(module, nn.Sequential) and
                not isinstance(module, nn.ModuleList) and
                not (module == model)):
            hooks.append(module.register_forward_hook(hook))

    if torch.cuda.is_available():
        dtype = torch.cuda.FloatTensor
        model = model.cuda()
    else:
        dtype = torch.FloatTensor
        model = model.cpu()

    # check if there are multiple inputs to the network
    if isinstance(input_size[0], (list, tuple)):
        x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]
    else:
        x = Variable(torch.rand(2, *input_size)).type(dtype)

    # print(type(x[0]))
    # create properties
    summary = OrderedDict()
    hooks = []
    # register hook
    model.apply(register_hook)
    # make a forward pass
    # print(x.shape)
    model(x)
    # remove these hooks
    for h in hooks:
        h.remove()

    print('----------------------------------------------------------------')
    line_new = '{:>20}  {:>25} {:>15}'.format('Layer (type)', 'Output Shape', 'Param #')
    print(line_new)
    print('================================================================')
    total_params = 0
    trainable_params = 0
    for layer in summary:
        # input_shape, output_shape, trainable, nb_params
        line_new = '{:>20}  {:>25} {:>15}'.format(layer, str(summary[layer]['output_shape']),
                                                  '{0:,}'.format(summary[layer]['nb_params']))
        total_params += summary[layer]['nb_params']
        if 'trainable' in summary[layer]:
            if summary[layer]['trainable'] == True:
                trainable_params += summary[layer]['nb_params']
        print(line_new)
    print('================================================================')
    print('Total params: {0:,}'.format(total_params))
    print('Trainable params: {0:,}'.format(trainable_params))
    print('Non-trainable params: {0:,}'.format(total_params - trainable_params))
    print('----------------------------------------------------------------')"
get inner html,https://github.com/maxpowel/scrapium/blob/bc12c425aa5978f953a87d05920ba0f61a00409c/scrapium/scrapium.py#L140-L142,3,"    def get_html(self, url):
        r = self.get(url)
        return self.html(r.text)"
output to html file,https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/examples/pipeline/custom_attr_methods.py#L43-L58,3,"def to_html(doc, output=""/tmp"", style=""dep""):
    """"""Doc method extension for saving the current state as a displaCy
    visualization.
    """"""
    # generate filename from first six non-punct tokens
    file_name = ""-"".join([w.text for w in doc[:6] if not w.is_punct]) + "".html""
    html = displacy.render(doc, style=style, page=True)  # render markup
    if output is not None:
        output_path = Path(output)
        if not output_path.exists():
            output_path.mkdir()
        output_file = Path(output) / file_name
        output_file.open(""w"", encoding=""utf-8"").write(html)  # save to file
        print(""Saved HTML to {}"".format(output_file))
    else:
        print(html)"
confusion matrix,https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/ml_tools/eolearn/ml_tools/validator.py#L201-L208,3,"    def confusion_matrix(self):
        """"""
        Returns the normalised confusion matrix
        """"""
        confusion_matrix = self.pixel_classification_sum.astype(np.float)
        confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T

        return confusion_matrix * 100.0"
heatmap from 3d coordinates,https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L110-L158,3,"    def draw(self, size=None, cmap=""jet""):
        """"""
        Render the heatmaps as RGB images.

        Parameters
        ----------
        size : None or float or iterable of int or iterable of float, optional
            Size of the rendered RGB image as ``(height, width)``.
            See :func:`imgaug.imgaug.imresize_single_image` for details.
            If set to None, no resizing is performed and the size of the heatmaps array is used.

        cmap : str or None, optional
            Color map of ``matplotlib`` to use in order to convert the heatmaps to RGB images.
            If set to None, no color map will be used and the heatmaps will be converted
            to simple intensity maps.

        Returns
        -------
        heatmaps_drawn : list of (H,W,3) ndarray
            Rendered heatmaps. One per heatmap array channel. Dtype is uint8.

        """"""
        heatmaps_uint8 = self.to_uint8()
        heatmaps_drawn = []

        for c in sm.xrange(heatmaps_uint8.shape[2]):
            # c:c+1 here, because the additional axis is needed by imresize_single_image
            heatmap_c = heatmaps_uint8[..., c:c+1]

            if size is not None:
                heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=""nearest"")
            else:
                heatmap_c_rs = heatmap_c
            heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0

            if cmap is not None:
                # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)
                import matplotlib.pyplot as plt

                cmap_func = plt.get_cmap(cmap)
                heatmap_cmapped = cmap_func(heatmap_c_rs)
                heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)
            else:
                heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3))

            heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)

            heatmaps_drawn.append(heatmap_cmapped)
        return heatmaps_drawn"
readonly array,https://github.com/nyaruka/smartmin/blob/488a676a4960555e4d216a7b95d6e01a4ad4efd8/smartmin/views.py#L914-L924,3,"    def derive_readonly(self):
        """"""
        Figures out what fields should be readonly.  We iterate our field_config to find all
        that have a readonly of true
        """"""
        readonly = list(self.readonly)
        for key, value in self.field_config.items():
            if 'readonly' in value and value['readonly']:
                readonly.append(key)

        return readonly"
get current process id,https://github.com/Jaymon/prom/blob/b7ad2c259eca198da03e1e4bc7d95014c168c361/prom/query.py#L1458-L1464,3,"    def process_id(self):
        ret = """"
        if thread:
            f = getattr(os, 'getpid', None)
            if f:
                ret = str(f())
        return ret"
copying a file to a path,https://github.com/klen/starter/blob/24a65c10d4ac5a9ca8fc1d8b3d54b3fb13603f5f/starter/core.py#L52-L58,3,"    def copy_file(self, from_path, to_path):
        """""" Copy file. """"""
        if not op.exists(op.dirname(to_path)):
            self.make_directory(op.dirname(to_path))

        shutil.copy(from_path, to_path)
        logging.debug('File copied: {0}'.format(to_path))"
positions of substrings in string,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491,3,"def findAllSubstrings(string, substring):
    """""" Returns a list of all substring starting positions in string or an empty
    list if substring is not present in string.

    :param string: a template string
    :param substring: a string, which is looked for in the ``string`` parameter.

    :returns: a list of substring starting positions in the template string
    """"""
    #TODO: solve with regex? what about '.':
    #return [m.start() for m in re.finditer('(?='+substring+')', string)]
    start = 0
    positions = []
    while True:
        start = string.find(substring, start)
        if start == -1:
            break
        positions.append(start)
        #+1 instead of +len(substring) to also find overlapping matches
        start += 1
    return positions"
how to get current date,https://github.com/airbus-cert/mispy/blob/6d523d6f134d2bd38ec8264be74e73b68403da65/mispy/misp.py#L595-L605,3,"    def date(self):
        """"""
        Getter/setter for the date member.

        The setter can take a string or a :meth:`datetime.datetime` and will do the
        appropriate transformation.

        """"""
        if self._date:
            return self._date
        return datetime.datetime.now().strftime('%Y-%m-%d')"
html entities replace,https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/repl.py#L65-L74,3,"def replace(html, replacements=None):
    """"""Performs replacements on given HTML string.""""""
    if not replacements:
        return html  # no replacements
    html = HTMLFragment(html)

    for r in replacements:
        r.replace(html)

    return unicode(html)"
matrix multiply,https://github.com/churchill-lab/emase/blob/ae3c6955bb175c1dec88dbf9fac1a7dcc16f4449/emase/Sparse3DMatrix.py#L130-L157,3,"    def __mul__(self, other):
        if self.finalized:
            dmat = self.__class__()
            dmat.shape = self.shape
            if isinstance(other, Sparse3DMatrix):  # element-wise multiplication between same kind
                if other.finalized:
                    for hid in xrange(self.shape[1]):
                        dmat.data.append(self.data[hid].multiply(other.data[hid]))
                else:
                    raise RuntimeError('Both matrices must be finalized.')
            elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)):  # matrix-matrix multiplication
                for hid in xrange(self.shape[1]):
                    dmat.data.append(self.data[hid] * other)
                dmat.shape = (other.shape[1], self.shape[1], self.shape[2])
            elif isinstance(other, (coo_matrix, lil_matrix)):              # matrix-matrix multiplication
                other_csc = other.tocsc()
                for hid in xrange(self.shape[1]):
                    dmat.data.append(self.data[hid] * other_csc)
                dmat.shape = (other_csc.shape[1], self.shape[1], self.shape[2])
            elif isinstance(other, Number):  # rescaling of matrix
                for hid in xrange(self.shape[1]):
                    dmat.data.append(self.data[hid] * other)
            else:
                raise TypeError('This operator is not supported between the given types.')
            dmat.finalized = True
            return dmat
        else:
            raise RuntimeError('The original matrix must be finalized.')"
normal distribution,https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/src/sdk/pynni/nni/curvefitting_assessor/model_factory.py#L204-L221,3,"    def normal_distribution(self, pos, sample):
        """"""returns the value of normal distribution, given the weight's sample and target position
        
        Parameters
        ----------
        pos: int
            the epoch number of the position you want to predict
        sample: list
            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}

        Returns
        -------
        float
            the value of normal distribution
        """"""
        curr_sigma_sq = self.sigma_sq(sample)
        delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)
        return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))"
get current observable value,https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/observable/decorator.py#L114-L143,3,"    def _set_observable(self, observable_name, new_value):
        
        ## check old value
        if self._has_value(observable_name):
            old_value = self.new_value(observable_name)
            
            ## set only if different value
            must_set = np.any(new_value != old_value)
            if must_set:
                
                ## if values are observable_names with observers call associated observers of sub observable_names
                if isinstance(old_value, Observable) and isinstance(new_value, Observable):
                    ## copy observer
                    old_value.copy_observers_to(new_value)
                    
                    ## notify old observer
                    old_value._notify_observers(observable_name=None, include_everything_observers=True)
                    for observable_name in old_value._observers.keys():
                        old_has_value = old_value._has_value(observable_name)
                        new_has_value = new_value._has_value(observable_name)
                        if old_has_value != new_has_value or (old_has_value and new_has_value and np.any(old_value._get_value(observable_name) != new_value._get_value(observable_name))):
                            old_value._notify_observers(observable_name=observable_name, include_everything_observers=False)
        else:
            must_set = True
        
            
        ## set new observable_name value and call observer
        if must_set:
            self._set_value(observable_name, new_value)
            self._notify_observers(observable_name)"
extracting data from a text file,https://github.com/joshburnett/scanf/blob/52f8911581c1590a3dcc6f17594eeb7b39716d42/scanf.py#L158-L184,3,"def extractdata(pattern, text=None, filepath=None):
    """"""
    Read through an entire file or body of text one line at a time. Parse each line that matches the supplied
    pattern string and ignore the rest.

    If *text* is supplied, it will be parsed according to the *pattern* string.
    If *text* is not supplied, the file at *filepath* will be opened and parsed.
    """"""
    y = []
    if text is None:
        textsource = open(filepath, 'r')
    else:
        textsource = text.splitlines()

    for line in textsource:
        match = scanf(pattern, line)
        if match:
            if len(y) == 0:
                y = [[s] for s in match]
            else:
                for i, ydata in enumerate(y):
                    ydata.append(match[i])

    if text is None:
        textsource.close()

    return y"
convert a utc time to epoch,https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149,3,"    def _dt_to_epoch(self, dt):
        """"""
        Convert a offset-aware datetime to POSIX time.
        """"""
        if PY2:
            # The input datetime is from botocore unmarshalling and it is
            # offset-aware so the timedelta of subtracting this time
            # to 01/01/1970 using the same tzinfo gives us
            # Unix Time (also known as POSIX Time).
            time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)
            return int(time_delta.total_seconds())
        else:
            # Added in python 3.3+ and directly returns POSIX time.
            return int(dt.timestamp())"
convert decimal to hex,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/textio.py#L121-L140,3,"    def hexadecimal(token):
        """"""
        Convert a strip of hexadecimal numbers into binary data.

        @type  token: str
        @param token: String to parse.

        @rtype:  str
        @return: Parsed string value.
        """"""
        token = ''.join([ c for c in token if c.isalnum() ])
        if len(token) % 2 != 0:
            raise ValueError(""Missing characters in hex data"")
        data = ''
        for i in compat.xrange(0, len(token), 2):
            x = token[i:i+2]
            d = int(x, 16)
            s = struct.pack('<B', d)
            data += s
        return data"
parse query string in url,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95,3,"def strip_url_params3(url, strip=None):
    if not strip: strip = []
    
    parse = urllib.parse.urlparse(url)
    query = urllib.parse.parse_qs(parse.query)
    
    query = {k: v[0] for k, v in query.items() if k not in strip}
    query = urllib.parse.urlencode(query)
    new = parse._replace(query=query)
    
    return new.geturl()"
buffered file reader read text,https://github.com/hyde/fswrap/blob/41e4ad6f7e9ba73eabe61bd97847cd284e3edbd2/fswrap.py#L282-L289,3,"    def read_all(self, encoding='utf-8'):
        """"""
        Reads from the file and returns the content as a string.
        """"""
        logger.info(""Reading everything from %s"" % self)
        with codecs.open(self.path, 'r', encoding) as fin:
            read_text = fin.read()
        return read_text"
converting uint8 array to image,https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L1058-L1069,3,"    def uint8_to_uint32(self, element):
        img = np.dstack([element.dimension_values(d, flat=False)
                         for d in element.vdims])
        if img.shape[2] == 3: # alpha channel not included
            alpha = np.ones(img.shape[:2])
            if img.dtype.name == 'uint8':
                alpha = (alpha*255).astype('uint8')
            img = np.dstack([img, alpha])
        if img.dtype.name != 'uint8':
            img = (img*255).astype(np.uint8)
        N, M, _ = img.shape
        return img.view(dtype=np.uint32).reshape((N, M))"
map to json,https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/server.py#L25-L53,3,"def maps_json():
    """"""
    Generates a json object which serves as bridge between
    the web interface and the map source collection.

    All attributes relevant for openlayers are converted into
    JSON and served through this route.

    Returns:
        Response: All map sources as JSON object.
    """"""
    map_sources = {
        id: {
                ""id"": map_source.id,
                ""name"": map_source.name,
                ""folder"": map_source.folder,
                ""min_zoom"": map_source.min_zoom,
                ""max_zoom"": map_source.max_zoom,
                ""layers"": [
                    {
                        ""min_zoom"": layer.min_zoom,
                        ""max_zoom"": layer.max_zoom,
                        ""tile_url"": layer.tile_url.replace(""$"", """"),
                    } for layer in map_source.layers
                    ]

            } for id, map_source in app.config[""mapsources""].items()
        }
    return jsonify(map_sources)"
convert string to number,https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L546-L556,3,"    def __convert_num(number):
        """"""
        All path items are automatically strings. If you think it's an int or float, this attempts to convert it.
        :param str number:
        :return float or str:
        """"""
        try:
            return float(number)
        except ValueError as e:
            logger_noaa_lpd.warn(""convert_num: ValueError: {}"".format(e))
            return number"
nelder mead optimize,https://github.com/zblz/naima/blob/d6a6781d73bf58fd8269e8b0e3b70be22723cd5b/naima/extern/minimize.py#L66-L67,3,"def minimize(func, x0, args=(), options={}, method=None):
    return _minimize_neldermead(func, x0, args=args, **options)"
sorting multiple arrays based on another arrays sorted order,https://github.com/JoseAntFer/pyny3d/blob/fb81684935a24f7e50c975cb4383c81a63ab56df/pyny3d/utils.py#L8-L33,3,"def sort_numpy(array, col=0, order_back=False):
    """"""
    Sorts the columns for an entire ``ndarrray`` according to sorting
    one of them.
    
    :param array: Array to sort.
    :type array: ndarray
    :param col: Master column to sort.
    :type col: int
    :param order_back: If True, also returns the index to undo the
        new order.
    :type order_back: bool
    :returns: sorted_array or [sorted_array, order_back]
    :rtype: ndarray, list
    """"""
    x = array[:,col]
    sorted_index = np.argsort(x, kind = 'quicksort')
    sorted_array = array[sorted_index]
    
    if not order_back:
        return sorted_array
    else:
        n_points = sorted_index.shape[0]
        order_back = np.empty(n_points, dtype=int)
        order_back[sorted_index] = np.arange(n_points)
        return [sorted_array, order_back]"
find int in string,https://github.com/fdb/aufmachen/blob/f2986a0cf087ac53969f82b84d872e3f1c6986f4/aufmachen/websites/immoweb.py#L115-L130,3,"def find_number(regex, s):
    """"""Find a number using a given regular expression.
    If the string cannot be found, returns None.
    The regex should contain one matching group, 
    as only the result of the first group is returned.
    The group should only contain numeric characters ([0-9]+).
    
    s - The string to search.
    regex - A string containing the regular expression.
    
    Returns an integer or None.
    """"""
    result = find_string(regex, s)
    if result is None:
        return None
    return int(result)"
initializing array,https://github.com/mbakker7/timml/blob/91e99ad573cb8a9ad8ac1fa041c3ca44520c2390/timml/linesink.py#L567-L595,3,"    def initialize(self):
        for ls in self.lslist:
            ls.initialize()
        # Same order for all elements in string
        self.ncp = self.nls * self.lslist[0].ncp
        self.nparam = self.nls * self.lslist[0].nparam
        self.nunknowns = self.nparam
        self.xls = np.empty((self.nls, 2))
        self.yls = np.empty((self.nls, 2))
        for i, ls in enumerate(self.lslist):
            self.xls[i, :] = [ls.x1, ls.x2]
            self.yls[i, :] = [ls.y1, ls.y2]
        if self.aq is None:
            self.aq = self.model.aq.find_aquifer_data(self.lslist[0].xc,
                                                      self.lslist[0].yc)
        self.parameters = np.zeros((self.nparam, 1))
        # As parameters are only stored for the element not the list,
        # we need to combine the following
        self.xc = np.array([ls.xc for ls in self.lslist]).flatten()
        self.yc = np.array([ls.yc for ls in self.lslist]).flatten()
        self.xcin = np.array([ls.xcin for ls in self.lslist]).flatten()
        self.ycin = np.array([ls.ycin for ls in self.lslist]).flatten()
        self.xcout = np.array([ls.xcout for ls in self.lslist]).flatten()
        self.ycout = np.array([ls.ycout for ls in self.lslist]).flatten()
        self.cosnorm = np.array([ls.cosnorm for ls in self.lslist]).flatten()
        self.sinnorm = np.array([ls.sinnorm for ls in self.lslist]).flatten()
        self.aqin = self.model.aq.find_aquifer_data(self.xcin[0], self.ycin[0])
        self.aqout = self.model.aq.find_aquifer_data(self.xcout[0],
                                                     self.ycout[0])"
read properties file,https://github.com/crs4/pydoop/blob/f375be2a06f9c67eaae3ce6f605195dbca143b2b/pydoop/__init__.py#L183-L193,3,"def read_properties(fname):
    parser = configparser.SafeConfigParser()
    parser.optionxform = str  # preserve key case
    try:
        with open(fname) as f:
            parser_read(parser, AddSectionWrapper(f))
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
        return None  # compile time, prop file is not there
    return dict(parser.items(AddSectionWrapper.SEC_NAME))"
how to empty array,https://github.com/openfisca/openfisca-core/blob/92ce9396e29ae5d9bac5ea604cfce88517c6b35c/openfisca_core/variables.py#L427-L433,3,"    def default_array(self, array_size):
        array = np.empty(array_size, dtype = self.dtype)
        if self.value_type == Enum:
            array.fill(self.default_value.index)
            return EnumArray(array, self.possible_values)
        array.fill(self.default_value)
        return array"
how to determine a string is a valid word,https://github.com/phoopy/phoopy-console/blob/d38ec0eb952e79239699a0f855c07437a34024b0/phoopy/console/helper/string_helper.py#L41-L43,3,"    def get_most_used_words(words, stopwords):
        valid_words = [word.lower() for word in words if StringHelper.is_valid_word(word, stopwords)]
        return dict(Counter(valid_words).most_common(5))"
fuzzy match ranking,https://github.com/hearsaycorp/normalize/blob/8b36522ddca6d41b434580bd848f3bdaa7a999c8/normalize/diff.py#L547-L588,3,"def _fuzzy_match(set_a, set_b):
    seen = dict()
    scores = list()

    # Yes, this is O(n.m), but python's equality operator is
    # fast for hashable types.
    for a_pk_seq, b_pk_seq in product(set_a, set_b):
        a_pk, a_seq = a_pk_seq
        b_pk, b_seq = b_pk_seq
        if (a_pk, b_pk) in seen:
            if seen[a_pk, b_pk][0]:
                score = list(seen[a_pk, b_pk])
                scores.append(score + [a_pk_seq, b_pk_seq])
        else:
            match = 0
            common = min((len(a_pk), len(b_pk)))
            no_match = max((len(a_pk), len(b_pk))) - common
            for i in range(0, common):
                if a_pk[i] == b_pk[i]:
                    if not _nested_falsy(a_pk[i]):
                        match += 1
                else:
                    no_match += 1
            seen[a_pk, b_pk] = (match, no_match)
            if match:
                scores.append([match, no_match, a_pk_seq, b_pk_seq])

    remaining_a = set(set_a)
    remaining_b = set(set_b)

    for match, no_match, a_pk_seq, b_pk_seq in sorted(
        scores,
        key=lambda x: x[0] - x[1],
        reverse=True,
    ):
        if a_pk_seq in remaining_a and b_pk_seq in remaining_b:
            remaining_a.remove(a_pk_seq)
            remaining_b.remove(b_pk_seq)
            yield a_pk_seq, b_pk_seq

        if not remaining_a or not remaining_b:
            break"
html encode string,https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111,3,"    def html(self, html: str) -> None:
        self._html = html.encode(self.encoding)"
priority queue,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49,3,"    def push(self, item, priority=None):
        """"""Push the item in the priority queue.
        if priority is not given, priority is set to the value of item.
        """"""
        priority = item if priority is None else priority
        node = PriorityQueueNode(item, priority)
        for index, current in enumerate(self.priority_queue_list):
            if current.priority < node.priority:
                self.priority_queue_list.insert(index, node)
                return
        # when traversed complete queue
        self.priority_queue_list.append(node)"
deducting the median from each column,https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/backends/pandas/query_compiler.py#L1251-L1263,3,"    def median(self, **kwargs):
        """"""Returns median of each column or row.

        Returns:
            A new QueryCompiler object containing the median of each column or row.
        """"""
        if self._is_transposed:
            kwargs[""axis""] = kwargs.get(""axis"", 0) ^ 1
            return self.transpose().median(**kwargs)
        # Pandas default is 0 (though not mentioned in docs)
        axis = kwargs.get(""axis"", 0)
        func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs)
        return self._full_axis_reduce(axis, func)"
sort string list,https://github.com/qacafe/cdrouter.py/blob/aacf2c6ab0b987250f7b1892f4bba14bb2b7dbe5/cdrouter/cdrouter.py#L282-L288,3,"    def list(self, base, filter=None, type=None, sort=None, limit=None, page=None, format=None): # pylint: disable=redefined-builtin
        if sort != None:
            if not isinstance(sort, list):
                sort = [sort]
            sort = ','.join(sort)
        return self.get(base, params={'filter': filter, 'type': type, 'sort': sort, 'limit': limit,
                                      'page': page, 'format': format})"
reading element from html - <td>,https://github.com/atlassian-api/atlassian-python-api/blob/540d269905c3e7547b666fe30c647b2d512cf358/atlassian/utils.py#L80-L103,3,"def html_row_with_ordered_headers(data, headers):
    """"""
    >>> headers = ['administrators', 'key', 'leader', 'project']
    >>> data = {'key': 'DEMO', 'project': 'Demonstration', 'leader': 'leader@example.com', 'administrators': ['admin1@example.com', 'admin2@example.com']}
    >>> html_row_with_ordered_headers(data, headers)
    '\\n\\t<tr><td><ul><li><a href=""mailto:admin1@example.com"">admin1@example.com</a></li><li><a href=""mailto:admin2@example.com"">admin2@example.com</a></li></ul></td><td>DEMO</td><td>leader@example.com</td><td>Demonstration</td></tr>'
    >>> headers = ['key', 'project', 'leader', 'administrators']
    >>> html_row_with_ordered_headers(data, headers)
    '\\n\\t<tr><td>DEMO</td><td>Demonstration</td><td>leader@example.com</td><td><ul><li><a href=""mailto:admin1@example.com"">admin1@example.com</a></li><li><a href=""mailto:admin2@example.com"">admin2@example.com</a></li></ul></td></tr>'
    """"""
    html = '\n\t<tr>'

    for header in headers:
        element = data[header]

        if isinstance(element, list):
            element = html_list(element)

        if is_email(element):
            element = html_email(element)

        html += '<td>{}</td>'.format(element)

    return html + '</tr>'"
save list to file,https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/cfgpars.py#L752-L790,3,"    def saveParList(self, *args, **kw):
        """"""Write parameter data to filename (string or filehandle)""""""
        if 'filename' in kw:
            filename = kw['filename']
        if not filename:
            filename = self.getFilename()
        if not filename:
            raise ValueError(""No filename specified to save parameters"")

        if hasattr(filename,'write'):
            fh = filename
            absFileName = os.path.abspath(fh.name)
        else:
            absFileName = os.path.expanduser(filename)
            absDir = os.path.dirname(absFileName)
            if len(absDir) and not os.path.isdir(absDir): os.makedirs(absDir)
            fh = open(absFileName,'w')
        numpars = len(self.__paramList)
        if self._forUseWithEpar: numpars -= 1
        if not self.final_comment: self.final_comment = [''] # force \n at EOF
        # Empty the ConfigObj version of section.defaults since that is based
        # on an assumption incorrect for us, and override with our own list.
        # THIS IS A BIT OF MONKEY-PATCHING!  WATCH FUTURE VERSION CHANGES!
        # See Trac ticket #762.
        while len(self.defaults):
            self.defaults.pop(-1) # empty it, keeping ref
        for key in self._neverWrite:
            self.defaults.append(key)
        # Note also that we are only overwriting the top/main section's
        # ""defaults"" list, but EVERY [sub-]section has such an attribute...

        # Now write to file, delegating work to ConfigObj (note that ConfigObj
        # write() skips any items listed by name in the self.defaults list)
        self.write(fh)
        fh.close()
        retval = str(numpars) + "" parameters written to "" + absFileName
        self.filename = absFileName # reset our own ConfigObj filename attr
        self.debug('Keys not written: '+str(self.defaults))
        return retval"
json to xml conversion,https://github.com/codeforamerica/three/blob/67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0/three/core.py#L158-L170,3,"    def convert(self, content, conversion):
        """"""Convert content to Python data structures.""""""
        if not conversion:
            data = content
        elif self.format == 'json':
            data = json.loads(content)
        elif self.format == 'xml':
            content = xml(content)
            first = list(content.keys())[0]
            data = content[first]
        else:
            data = content
        return data"
hash set for counting distinct elements,https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2510-L2520,2,"    def distinct_counts(self):
        """"""Return counts for each distinct haplotype.""""""

        # hash the haplotypes
        k = [hash(self.values[:, i].tobytes()) for i in range(self.shape[1])]

        # count and sort
        # noinspection PyArgumentList
        counts = sorted(collections.Counter(k).values(), reverse=True)

        return np.asarray(counts)"
concatenate several file remove header lines,https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/mdf.py#L1821-L2190,2,"    def concatenate(files, version=""4.10"", sync=True, add_samples_origin=False, **kwargs):
        """""" concatenates several files. The files
        must have the same internal structure (same number of groups, and same
        channels in each group)

        Parameters
        ----------
        files : list | tuple
            list of *MDF* file names or *MDF* instances
        version : str
            merged file version
        sync : bool
            sync the files based on the start of measurement, default *True*
        add_samples_origin : bool
            option to create a new ""__samples_origin"" channel that will hold
            the index of the measurement from where each timestamp originated

        Returns
        -------
        concatenate : MDF
            new *MDF* object with concatenated channels

        Raises
        ------
        MdfException : if there are inconsistencies between the files

        """"""
        if not files:
            raise MdfException(""No files given for merge"")

        callback = kwargs.get(""callback"", None)
        if callback:
            callback(0, 100)

        mdf_nr = len(files)

        versions = []
        if sync:
            timestamps = []
            for file in files:
                if isinstance(file, MDF):
                    timestamps.append(file.header.start_time)
                    versions.append(file.version)
                else:
                    with open(file, ""rb"") as mdf:
                        mdf.seek(64)
                        blk_id = mdf.read(2)
                        if blk_id == b""HD"":
                            header = HeaderV3
                            versions.append(""3.00"")
                        else:
                            versions.append(""4.00"")
                            blk_id += mdf.read(2)
                            if blk_id == b""##HD"":
                                header = HeaderV4
                            else:
                                raise MdfException(f'""{file}"" is not a valid MDF file')

                        header = header(address=64, stream=mdf)

                        timestamps.append(header.start_time)

            try:
                oldest = min(timestamps)
            except TypeError:
                timestamps = [
                    timestamp.astimezone(timezone.utc)
                    for timestamp in timestamps
                ]
                oldest = min(timestamps)

            offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps]
            offsets = [offset if offset > 0 else 0 for offset in offsets]

        else:
            file = files[0]
            if isinstance(file, MDF):
                oldest = file.header.start_time
                versions.append(file.version)
            else:
                with open(file, ""rb"") as mdf:
                    mdf.seek(64)
                    blk_id = mdf.read(2)
                    if blk_id == b""HD"":
                        versions.append(""3.00"")
                        header = HeaderV3
                    else:
                        versions.append(""4.00"")
                        blk_id += mdf.read(2)
                        if blk_id == b""##HD"":
                            header = HeaderV4
                        else:
                            raise MdfException(f'""{file}"" is not a valid MDF file')

                    header = header(address=64, stream=mdf)

                    oldest = header.start_time

            offsets = [0 for _ in files]

        version = validate_version_argument(version)

        merged = MDF(version=version, callback=callback)

        merged.header.start_time = oldest

        encodings = []
        included_channel_names = []

        if add_samples_origin:
            origin_conversion = {}
            for i, mdf in enumerate(files):
                origin_conversion[f'val_{i}'] = i
                if isinstance(mdf, MDF):
                    origin_conversion[f'text_{i}'] = str(mdf.name)
                else:
                    origin_conversion[f'text_{i}'] = str(mdf)
            origin_conversion = from_dict(origin_conversion)

        for mdf_index, (offset, mdf) in enumerate(zip(offsets, files)):
            if not isinstance(mdf, MDF):
                mdf = MDF(mdf)

            try:
                for can_id, info in mdf.can_logging_db.items():
                    if can_id not in merged.can_logging_db:
                        merged.can_logging_db[can_id] = {}
                    merged.can_logging_db[can_id].update(info)
            except AttributeError:
                pass

            if mdf_index == 0:
                last_timestamps = [None for gp in mdf.groups]
                groups_nr = len(mdf.groups)

            cg_nr = -1

            for i, group in enumerate(mdf.groups):
                included_channels = mdf._included_channels(i)

                if mdf_index == 0:
                    included_channel_names.append(
                        [group.channels[k].name for k in included_channels]
                    )
                else:
                    names = [group.channels[k].name for k in included_channels]
                    if names != included_channel_names[i]:
                        if sorted(names) != sorted(included_channel_names[i]):
                            raise MdfException(f""internal structure of file {mdf_index} is different"")
                        else:
                            logger.warning(
                                f'Different channel order in channel group {i} of file {mdf_index}.'
                                ' Data can be corrupted if the there are channels with the same '
                                'name in this channel group'
                            )
                            included_channels = [
                                mdf._validate_channel_selection(
                                    name=name_,
                                    group=i,
                                )[1]
                                for name_ in included_channel_names[i]
                            ]

                if included_channels:
                    cg_nr += 1
                else:
                    continue
                channels_nr = len(group.channels)

                y_axis = MERGE

                idx = np.searchsorted(CHANNEL_COUNT, channels_nr, side=""right"") - 1
                if idx < 0:
                    idx = 0
                read_size = y_axis[idx]

                idx = 0
                last_timestamp = last_timestamps[i]
                first_timestamp = None
                original_first_timestamp = None

                if read_size:
                    mdf.configure(read_fragment_size=int(read_size))

                parents, dtypes = mdf._prepare_record(group)

                data = mdf._load_data(group)

                for fragment in data:

                    if dtypes.itemsize:
                        group.record = np.core.records.fromstring(
                            fragment[0], dtype=dtypes
                        )
                    else:
                        group.record = None

                    if mdf_index == 0 and idx == 0:
                        encodings_ = []
                        encodings.append(encodings_)
                        signals = []
                        for j in included_channels:
                            sig = mdf.get(
                                group=i,
                                index=j,
                                data=fragment,
                                raw=True,
                                ignore_invalidation_bits=True,
                                copy_master=False,
                            )

                            if version < ""4.00"":
                                if sig.samples.dtype.kind == ""S"":
                                    encodings_.append(sig.encoding)
                                    strsig = mdf.get(
                                        group=i,
                                        index=j,
                                        samples_only=True,
                                        ignore_invalidation_bits=True,
                                    )[0]
                                    sig.samples = sig.samples.astype(strsig.dtype)
                                    del strsig
                                    if sig.encoding != ""latin-1"":

                                        if sig.encoding == ""utf-16-le"":
                                            sig.samples = (
                                                sig.samples.view(np.uint16)
                                                .byteswap()
                                                .view(sig.samples.dtype)
                                            )
                                            sig.samples = encode(
                                                decode(sig.samples, ""utf-16-be""),
                                                ""latin-1"",
                                            )
                                        else:
                                            sig.samples = encode(
                                                decode(sig.samples, sig.encoding),
                                                ""latin-1"",
                                            )
                                else:
                                    encodings_.append(None)

                            if not sig.samples.flags.writeable:
                                sig.samples = sig.samples.copy()
                            signals.append(sig)

                        if signals and len(signals[0]):
                            if offset > 0:
                                timestamps = sig[0].timestamps + offset
                                for sig in signals:
                                    sig.timestamps = timestamps
                            last_timestamp = signals[0].timestamps[-1]
                            first_timestamp = signals[0].timestamps[0]
                            original_first_timestamp = first_timestamp

                        if add_samples_origin:
                            if signals:
                                _s = signals[-1]
                                signals.append(
                                    Signal(
                                        samples=np.ones(len(_s), dtype='<u2')*mdf_index,
                                        timestamps=_s.timestamps,
                                        conversion=origin_conversion,
                                        name='__samples_origin',
                                    )
                                )
                                _s = None

                        if signals:
                            merged.append(signals, common_timebase=True)
                            try:
                                if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:
                                    merged.groups[-1].channel_group.flags = group.channel_group.flags
                                    merged.groups[-1].channel_group.acq_name = group.channel_group.acq_name
                                    merged.groups[-1].channel_group.acq_source = group.channel_group.acq_source
                                    merged.groups[-1].channel_group.comment = group.channel_group.comment
                            except AttributeError:
                                pass
                        else:
                            break
                        idx += 1
                    else:
                        master = mdf.get_master(i, fragment, copy_master=False)
                        _copied = False

                        if len(master):
                            if original_first_timestamp is None:
                                original_first_timestamp = master[0]
                            if offset > 0:
                                master = master + offset
                                _copied = True
                            if last_timestamp is None:
                                last_timestamp = master[-1]
                            else:
                                if last_timestamp >= master[0]:
                                    if len(master) >= 2:
                                        delta = master[1] - master[0]
                                    else:
                                        delta = 0.001
                                    if _copied:
                                        master -= master[0]
                                    else:
                                        master = master - master[0]
                                        _copied = True
                                    master += last_timestamp + delta
                                last_timestamp = master[-1]

                            signals = [(master, None)]

                            for k, j in enumerate(included_channels):
                                sig = mdf.get(
                                    group=i,
                                    index=j,
                                    data=fragment,
                                    raw=True,
                                    samples_only=True,
                                    ignore_invalidation_bits=True,
                                )

                                signals.append(sig)

                                if version < ""4.00"":
                                    encoding = encodings[i][k]
                                    samples = sig[0]
                                    if encoding:
                                        if encoding != ""latin-1"":

                                            if encoding == ""utf-16-le"":
                                                samples = (
                                                    samples.view(np.uint16)
                                                    .byteswap()
                                                    .view(samples.dtype)
                                                )
                                                samples = encode(
                                                    decode(samples, ""utf-16-be""),
                                                    ""latin-1"",
                                                )
                                            else:
                                                samples = encode(
                                                    decode(samples, encoding), ""latin-1""
                                                )
                                            sig.samples = samples


                            if signals:
                                if add_samples_origin:
                                    _s = signals[-1][0]
                                    signals.append(
                                        (np.ones(len(_s), dtype='<u2')*mdf_index, None)
                                    )
                                    _s = None
                                merged.extend(cg_nr, signals)

                            if first_timestamp is None:
                                first_timestamp = master[0]
                        idx += 1

                    group.record = None

                last_timestamps[i] = last_timestamp

            if callback:
                callback(i + 1 + mdf_index * groups_nr, groups_nr * mdf_nr)

            if MDF._terminate:
                return

            merged._transfer_events(mdf)

        return merged"
set file attrib hidden,https://github.com/mattharrison/rst2odp/blob/4adbf29b28c8207ec882f792ded07e98b1d3e7d0/odplib/preso.py#L170-L175,2,"def sub_el(parent, tag, attrib=None):
    attrib = attrib or {}
    tag = get_nstag(tag)
    attrib = update_attrib(attrib)
    el = et.SubElement(parent, tag, attrib)  # , nsmap=NAMESPACES)
    return el"
how to reverse a string,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L9-L14,2,"def reverse_words(string):
    arr = string.strip().split()  # arr is list of words
    n = len(arr)
    reverse(arr, 0, n-1)

    return "" "".join(arr)"
